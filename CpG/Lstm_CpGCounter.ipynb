{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3C7NfNYsOIJq",
   "metadata": {
    "id": "3C7NfNYsOIJq"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ozThEZghBQCu",
   "metadata": {
    "id": "ozThEZghBQCu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Sequence\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lYk4RjNbtRLX",
   "metadata": {
    "id": "lYk4RjNbtRLX"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8bRffsYvBP_N",
   "metadata": {
    "id": "8bRffsYvBP_N"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4zs9GC5uBP9L",
   "metadata": {
    "id": "4zs9GC5uBP9L"
   },
   "outputs": [],
   "source": [
    "# we prepared two datasets for training and evaluation\n",
    "# training data scale we set to 2048\n",
    "# we test on 512\n",
    "set_seed(13)\n",
    "def prepare_data(num_samples=100):\n",
    "    # prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    # step 1\n",
    "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
    "    \"\"\"\n",
    "    hint:\n",
    "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X \n",
    "        2. You first convert ids back to DNA sequence\n",
    "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
    "    \"\"\"\n",
    "    #step2 : # use intseq_to_dnaseq here to convert ids back to DNA seqs \n",
    "    temp = [list(intseq_to_dnaseq(i)) for i in X_dna_seqs_train] \n",
    "    \n",
    "    #step3 : # use count_cpgs here to generate labels with temp generated in step2 \n",
    "    y_dna_seqs = [count_cpgs(''.join(i)) for i in temp]\n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8NiW6JjtXR5",
   "metadata": {
    "id": "W8NiW6JjtXR5"
   },
   "source": [
    "## Data Set Class for Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "IF9AmmXxBP6o",
   "metadata": {
    "id": "IF9AmmXxBP6o"
   },
   "outputs": [],
   "source": [
    "# create data loader\n",
    "class DNASeqData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "train_dataset = DNASeqData(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float())\n",
    "test_dataset = DNASeqData(torch.from_numpy(test_x).float(), torch.from_numpy(test_y).float())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cwqR7YqtaFx",
   "metadata": {
    "id": "9cwqR7YqtaFx"
   },
   "source": [
    "## LSTM Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8TSHhMapBP4Y",
   "metadata": {
    "id": "8TSHhMapBP4Y"
   },
   "outputs": [],
   "source": [
    "# MODEL ARCHITECTURE \n",
    "class CpGDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, N_CLASSES, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,N_EMBEDDINGS):\n",
    "        super(CpGDetector, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(INPUT_SIZE,N_EMBEDDINGS)\n",
    "        self.num_classes = N_CLASSES\n",
    "        self.num_layers = N_LAYERS\n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=N_EMBEDDINGS, \n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=N_LAYERS\n",
    "        )\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE,self.num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.int()\n",
    "        embedded = self.embedding(x)  # adding embedding layer \n",
    "        output, _ = self.lstm(embedded) # passing the embedding vector through the lstm unit \n",
    "        output = self.drop(output[:, -1, :]) # taking the last hidden state after dropout \n",
    "        output = self.fc(self.relu(output))  # activation layer \n",
    "        return output.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Js-21e-Ktd73",
   "metadata": {
    "id": "Js-21e-Ktd73"
   },
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "jqZ-Pzz1BP1k",
   "metadata": {
    "id": "jqZ-Pzz1BP1k"
   },
   "outputs": [],
   "source": [
    "MODEL_PARAMETERS = {\n",
    "    'N_CLASSES':1,\n",
    "    'INPUT_SIZE':128,\n",
    "    'HIDDEN_SIZE':64,\n",
    "    'N_LAYERS':1,\n",
    "    'N_EMBEDDINGS':32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6UPt9XkzBPzf",
   "metadata": {
    "id": "6UPt9XkzBPzf"
   },
   "outputs": [],
   "source": [
    "set_seed(13)\n",
    "MODEL = CpGDetector(**MODEL_PARAMETERS)\n",
    "TRAINING_CONFIG = {\n",
    "    'N_EPOCHS':25,\n",
    "    'BATCH_SIZE':8,\n",
    "    'LEARNING_RATE':0.01,\n",
    "    'CRITERION':torch.nn.MSELoss()\n",
    "}\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=TRAINING_CONFIG['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1bHcOjO8BPw6",
   "metadata": {
    "id": "1bHcOjO8BPw6"
   },
   "outputs": [],
   "source": [
    "# creatin the torch data loader class for train and test\n",
    "set_seed(13)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=TRAINING_CONFIG['BATCH_SIZE'], shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=TRAINING_CONFIG['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "Vx26ltDqBPuU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vx26ltDqBPuU",
    "outputId": "fe5e9e6a-4bb7-4f2d-82f6-3061f0774585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1496.97855 Test loss : 323.32062\n",
      "Epoch: 1, Train loss: 1263.74828 Test loss : 315.11848\n",
      "Epoch: 2, Train loss: 1275.31169 Test loss : 306.82204\n",
      "Epoch: 3, Train loss: 1232.96073 Test loss : 306.41611\n",
      "Epoch: 4, Train loss: 1227.91676 Test loss : 307.04581\n",
      "Epoch: 5, Train loss: 1211.53485 Test loss : 297.24400\n",
      "Epoch: 6, Train loss: 1190.54235 Test loss : 282.44889\n",
      "Epoch: 7, Train loss: 1166.51572 Test loss : 283.83205\n",
      "Epoch: 8, Train loss: 1144.05296 Test loss : 293.17405\n",
      "Epoch: 9, Train loss: 1136.22175 Test loss : 292.88194\n",
      "Epoch: 10, Train loss: 1149.49469 Test loss : 269.87857\n",
      "Epoch: 11, Train loss: 1151.05677 Test loss : 293.91255\n",
      "Epoch: 12, Train loss: 1119.53765 Test loss : 274.79239\n",
      "Epoch: 13, Train loss: 1102.96695 Test loss : 277.45624\n",
      "Epoch: 14, Train loss: 1123.02735 Test loss : 273.58569\n",
      "Epoch: 15, Train loss: 1118.39870 Test loss : 278.23013\n",
      "Epoch: 16, Train loss: 1114.44511 Test loss : 268.97942\n",
      "Epoch: 17, Train loss: 1119.37647 Test loss : 291.22707\n",
      "Epoch: 18, Train loss: 1113.26796 Test loss : 276.31340\n",
      "Epoch: 19, Train loss: 1116.71435 Test loss : 272.99330\n",
      "Epoch: 20, Train loss: 1105.65080 Test loss : 286.37436\n",
      "Epoch: 21, Train loss: 1112.55073 Test loss : 273.56915\n",
      "Epoch: 22, Train loss: 1114.02040 Test loss : 277.92633\n",
      "Epoch: 23, Train loss: 1095.09848 Test loss : 269.26740\n",
      "Epoch: 24, Train loss: 1098.37955 Test loss : 269.97802\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "set_seed(13)\n",
    "for epoch in range(TRAINING_CONFIG['N_EPOCHS']):\n",
    "    EPOCH_LOSSES = 0 \n",
    "    for j,(x_train,y_train) in enumerate(train_data_loader):\n",
    "        outputs = MODEL(x_train)\n",
    "        OPTIMIZER.zero_grad()\n",
    "        y_train = y_train.unsqueeze(0)\n",
    "        loss = TRAINING_CONFIG['CRITERION'](outputs, y_train)\n",
    "        test_loss_total = 0 \n",
    "        with torch.inference_mode():\n",
    "            test_loss_total = 0 \n",
    "            for j,(x_test,y_test) in enumerate(test_data_loader):\n",
    "                test_output = MODEL(x_test)\n",
    "                y_test = y_test.unsqueeze(0)\n",
    "                test_loss = TRAINING_CONFIG['CRITERION'](test_output,y_test)\n",
    "                test_loss_total+=test_loss.item()\n",
    "                \n",
    "        EPOCH_LOSSES+=loss.item()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch: %d, Train loss: %1.5f Test loss : %1.5f\"  % (epoch, EPOCH_LOSSES,test_loss_total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "1x4jUYl9OPtw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x4jUYl9OPtw",
    "outputId": "eb90eea8-1abc-47c8-80d7-3709cf460e0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpGDetector(\n",
       "  (embedding): Embedding(128, 32)\n",
       "  (lstm): LSTM(32, 64)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dhYcl3pFthw1",
   "metadata": {
    "id": "dhYcl3pFthw1"
   },
   "source": [
    "## Model Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E6QBf4NTqPR8",
   "metadata": {
    "id": "E6QBf4NTqPR8"
   },
   "source": [
    "### Predict function for a single text/tensor input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "HCxNK3qQb_h3",
   "metadata": {
    "id": "HCxNK3qQb_h3"
   },
   "outputs": [],
   "source": [
    "def predict_full_length_seq(lstm_model,input_string,device,type='tensor'):\n",
    "    set_seed(13)\n",
    "    alphabet = 'NACGT'\n",
    "    dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "    int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "    if type!='tensor':\n",
    "        input_string_ints =  np.array([dna2int[i] for i in list(input_string)])\n",
    "        input_tensor = torch.from_numpy(input_string_ints).int()\n",
    "    else:\n",
    "        input_tensor = input_string\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor =  input_tensor.unsqueeze(0)\n",
    "        predictions = lstm_model(input_tensor)\n",
    "        return predictions.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cLbk7XmuqSBn",
   "metadata": {
    "id": "cLbk7XmuqSBn"
   },
   "source": [
    "### Testing the predict_full_length_seq function for a single test input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "547wVZBPd3BI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "547wVZBPd3BI",
    "outputId": "03f29089-f974-4847-db1d-aed04640fd09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.699720859527588"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_full_len_model =\"\"\"\n",
    "NACNCTTCGTGGANCAATNCAATAACGATCNNTAANNNACAGCGCGGANANACGCCATCNCNGTGNGCAGTNCNAATAGATATCCGCTGCCNCAAANCGGNTGTTAAATGACCTTTNTNTNNCCNCNCN\n",
    "\"\"\".replace('\\n','')\n",
    "test_sample_actual_output = count_cpgs(test_sample_full_len_model)\n",
    "test_sample_full_len_model,test_sample_actual_output\n",
    "predict_full_length_seq(MODEL,test_sample_full_len_model,'cpu',type='string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh_hSB0rZaq",
   "metadata": {
    "id": "1bh_hSB0rZaq"
   },
   "source": [
    "### Evaluating full length (128) model on Testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "I1RnPtSeb-yi",
   "metadata": {
    "id": "I1RnPtSeb-yi"
   },
   "outputs": [],
   "source": [
    "test_predictions_model_1 = []\n",
    "test_actuals_model_1 = []\n",
    "for i in zip(test_x,test_y):\n",
    "    input_tensor,input_target = i\n",
    "    input_tensor = torch.tensor(input_tensor)\n",
    "    input_target = torch.tensor(input_target)\n",
    "    model_prediction = predict_full_length_seq(MODEL,input_tensor,'cpu',type='tensor')\n",
    "    test_predictions_model_1.append(model_prediction)\n",
    "    actual = input_target.item()\n",
    "    test_actuals_model_1.append(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yU9yPNDOtprn",
   "metadata": {
    "id": "yU9yPNDOtprn"
   },
   "source": [
    "### Mean Absolute Error on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "oen7VHb2qvJt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oen7VHb2qvJt",
    "outputId": "ca2e3194-ffec-48de-fccb-1796bddc4cc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.604596383869648"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(test_predictions_model_1,test_actuals_model_1)\n",
    "# on average there is difference of 1.6 between actual CpG count and predicted CpG count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L9taZWi-ORS_",
   "metadata": {
    "id": "L9taZWi-ORS_"
   },
   "source": [
    "# Part 2: What if the DNA sequences are not the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hM2g6v_PtzlU",
   "metadata": {
    "id": "hM2g6v_PtzlU"
   },
   "source": [
    "## Data Preparation (Variable Sequence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d08cde66",
   "metadata": {
    "id": "d08cde66"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int = 128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int = 16, ub: int = 128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i + 2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "51cc912b",
   "metadata": {
    "id": "51cc912b"
   },
   "outputs": [],
   "source": [
    "# Alphabet helpers\n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"pad\": 0})\n",
    "int2dna.update({0: \"<pad>\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6f731665",
   "metadata": {
    "id": "6f731665"
   },
   "outputs": [],
   "source": [
    "# TODO complete the task based on the change\n",
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    temp =[[int2dna[j] for j in i] for i in X_dna_seqs_train]\n",
    "    y_dna_seqs =  [count_cpgs(''.join(i)) for i in temp]\n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "\n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c31d222e",
   "metadata": {
    "id": "c31d222e"
   },
   "outputs": [],
   "source": [
    "# creating the dataset class for data loader \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lists, labels): \n",
    "        self.lists = lists\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vBtlPUI6t9i-",
   "metadata": {
    "id": "vBtlPUI6t9i-"
   },
   "source": [
    "## LSTM Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5e5be25b",
   "metadata": {
    "id": "5e5be25b"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # adding embedding layer \n",
    "        output, _ = self.lstm(embedded) # passing the embedding vector through the lstm unit \n",
    "        output = self.drop(output[:, -1, :]) # taking the last hidden state after dropout \n",
    "        output = self.fc(self.relu(output))  # activation layer \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sWd_NWjiuCJt",
   "metadata": {
    "id": "sWd_NWjiuCJt"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ANfLVy7euGwd",
   "metadata": {
    "id": "ANfLVy7euGwd"
   },
   "source": [
    "### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "58b6e59e",
   "metadata": {
    "id": "58b6e59e"
   },
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "    'input_dim': 128,\n",
    "    'embedding_dim': 32,\n",
    "    'hidden_dim': 64,\n",
    "    'num_layers': 1,\n",
    "    'num_classes': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "badbccf2",
   "metadata": {
    "id": "badbccf2"
   },
   "outputs": [],
   "source": [
    "model = LSTM(**model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b4bd7ad4",
   "metadata": {
    "id": "b4bd7ad4"
   },
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    'n_epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.01,\n",
    "    'criterion': torch.nn.MSELoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6a8d1890",
   "metadata": {
    "id": "6a8d1890"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=training_config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lxMG2VXeuL-V",
   "metadata": {
    "id": "lxMG2VXeuL-V"
   },
   "source": [
    "### Creating Collate function for the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8f782652",
   "metadata": {
    "id": "8f782652"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    lengths = torch.LongTensor([len(seq) for seq in sequences])\n",
    "    packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "    return packed_sequences, torch.tensor(labels)\n",
    "\n",
    "\n",
    "train_x = [(torch.tensor(seq),torch.tensor(target)) for seq,target in zip(train_x,train_y)]\n",
    "test_x = [(torch.tensor(seq),torch.tensor(target)) for seq,target in zip(test_x,test_y)]\n",
    "# creating the torch data loader class for train and test\n",
    "train_data_loader = torch.utils.data.DataLoader(train_x, batch_size=training_config['batch_size'],collate_fn=collate_fn)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_x, batch_size=training_config['batch_size'],collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b1e0e7e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1e0e7e3",
    "outputId": "5c9432d9-0889-47dc-8497-8238019f0728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embedding): Embedding(128, 32)\n",
      "  (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kykKdt14uRDS",
   "metadata": {
    "id": "kykKdt14uRDS"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1eedbf22",
   "metadata": {
    "id": "1eedbf22"
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, loss_function, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        packed_sequence, targets = batch\n",
    "        data, lengths = pad_packed_sequence(packed_sequence, batch_first=True)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device).float()\n",
    "        targets = targets.unsqueeze(0).T\n",
    "        \n",
    "        predictions = model(data)\n",
    "        loss = loss_function(predictions, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "33bea2cb",
   "metadata": {
    "id": "33bea2cb"
   },
   "outputs": [],
   "source": [
    "def validate(model, data_loader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            packed_sequence, targets = batch\n",
    "            data, lengths = pad_packed_sequence(packed_sequence, batch_first=True)\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device).float()\n",
    "            targets = targets.unsqueeze(0).T\n",
    "\n",
    "            predictions = model(data)\n",
    "            loss = loss_function(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "be237452",
   "metadata": {
    "id": "be237452"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, loss_function, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, loss_function, optimizer, device)\n",
    "        valid_loss = validate(model, valid_loader, loss_function, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} ; Train Loss: {train_loss:.4f} ; Valid Loss: {valid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "eb443fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb443fdc",
    "outputId": "4ef9399f-0d4f-4969-caa6-23ff197bd8ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 ; Train Loss: 237.4198 ; Valid Loss: 64.0410\n",
      "Epoch 2/50 ; Train Loss: 237.3519 ; Valid Loss: 63.3353\n",
      "Epoch 3/50 ; Train Loss: 235.7609 ; Valid Loss: 62.4794\n",
      "Epoch 4/50 ; Train Loss: 237.8076 ; Valid Loss: 63.2557\n",
      "Epoch 5/50 ; Train Loss: 237.3387 ; Valid Loss: 62.9999\n",
      "Epoch 6/50 ; Train Loss: 236.7121 ; Valid Loss: 62.6256\n",
      "Epoch 7/50 ; Train Loss: 235.7550 ; Valid Loss: 62.2630\n",
      "Epoch 8/50 ; Train Loss: 233.6606 ; Valid Loss: 62.1105\n",
      "Epoch 9/50 ; Train Loss: 232.7535 ; Valid Loss: 63.1441\n",
      "Epoch 10/50 ; Train Loss: 232.5397 ; Valid Loss: 62.0602\n",
      "Epoch 11/50 ; Train Loss: 230.7555 ; Valid Loss: 61.5101\n",
      "Epoch 12/50 ; Train Loss: 228.1994 ; Valid Loss: 55.9649\n",
      "Epoch 13/50 ; Train Loss: 220.3994 ; Valid Loss: 58.2307\n",
      "Epoch 14/50 ; Train Loss: 213.6784 ; Valid Loss: 62.2084\n",
      "Epoch 15/50 ; Train Loss: 212.1491 ; Valid Loss: 59.7709\n",
      "Epoch 16/50 ; Train Loss: 223.0662 ; Valid Loss: 61.3179\n",
      "Epoch 17/50 ; Train Loss: 217.5468 ; Valid Loss: 63.6242\n",
      "Epoch 18/50 ; Train Loss: 223.5398 ; Valid Loss: 56.6123\n",
      "Epoch 19/50 ; Train Loss: 211.6780 ; Valid Loss: 56.2560\n",
      "Epoch 20/50 ; Train Loss: 209.2296 ; Valid Loss: 60.5851\n",
      "Epoch 21/50 ; Train Loss: 207.0874 ; Valid Loss: 54.6766\n",
      "Epoch 22/50 ; Train Loss: 209.3012 ; Valid Loss: 54.9735\n",
      "Epoch 23/50 ; Train Loss: 205.5589 ; Valid Loss: 54.6614\n",
      "Epoch 24/50 ; Train Loss: 209.3459 ; Valid Loss: 58.8417\n",
      "Epoch 25/50 ; Train Loss: 207.6404 ; Valid Loss: 54.3967\n",
      "Epoch 26/50 ; Train Loss: 205.0481 ; Valid Loss: 55.7400\n",
      "Epoch 27/50 ; Train Loss: 206.3380 ; Valid Loss: 56.0860\n",
      "Epoch 28/50 ; Train Loss: 208.2042 ; Valid Loss: 54.4177\n",
      "Epoch 29/50 ; Train Loss: 201.7621 ; Valid Loss: 56.7747\n",
      "Epoch 30/50 ; Train Loss: 204.6362 ; Valid Loss: 55.1775\n",
      "Epoch 31/50 ; Train Loss: 201.7460 ; Valid Loss: 54.7922\n",
      "Epoch 32/50 ; Train Loss: 198.5932 ; Valid Loss: 55.7963\n",
      "Epoch 33/50 ; Train Loss: 204.7112 ; Valid Loss: 54.6229\n",
      "Epoch 34/50 ; Train Loss: 203.8674 ; Valid Loss: 56.6616\n",
      "Epoch 35/50 ; Train Loss: 200.6368 ; Valid Loss: 56.3952\n",
      "Epoch 36/50 ; Train Loss: 199.4634 ; Valid Loss: 56.8393\n",
      "Epoch 37/50 ; Train Loss: 197.2217 ; Valid Loss: 56.6183\n",
      "Epoch 38/50 ; Train Loss: 199.2725 ; Valid Loss: 53.6923\n",
      "Epoch 39/50 ; Train Loss: 184.1194 ; Valid Loss: 43.4642\n",
      "Epoch 40/50 ; Train Loss: 163.4506 ; Valid Loss: 42.1932\n",
      "Epoch 41/50 ; Train Loss: 142.1501 ; Valid Loss: 27.6634\n",
      "Epoch 42/50 ; Train Loss: 135.4558 ; Valid Loss: 28.7769\n",
      "Epoch 43/50 ; Train Loss: 101.2849 ; Valid Loss: 22.5277\n",
      "Epoch 44/50 ; Train Loss: 93.9812 ; Valid Loss: 21.2106\n",
      "Epoch 45/50 ; Train Loss: 83.3487 ; Valid Loss: 18.4762\n",
      "Epoch 46/50 ; Train Loss: 74.9328 ; Valid Loss: 17.7624\n",
      "Epoch 47/50 ; Train Loss: 70.5293 ; Valid Loss: 13.5958\n",
      "Epoch 48/50 ; Train Loss: 66.4965 ; Valid Loss: 19.4533\n",
      "Epoch 49/50 ; Train Loss: 64.3544 ; Valid Loss: 13.2057\n",
      "Epoch 50/50 ; Train Loss: 59.7106 ; Valid Loss: 11.3470\n"
     ]
    }
   ],
   "source": [
    "set_seed(13)\n",
    "train_model(\n",
    "    model, \n",
    "    train_data_loader, \n",
    "    test_data_loader, \n",
    "    training_config['criterion'], \n",
    "    optimizer, \n",
    "    'cpu', \n",
    "    training_config['n_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "2c60246a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c60246a",
    "outputId": "3bdc66ca-e0a4-4c12-f211-0ffa216388fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(128, 32)\n",
       "  (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-K8a-nMwuYQp",
   "metadata": {
    "id": "-K8a-nMwuYQp"
   },
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pCrWib7pTFW2",
   "metadata": {
    "id": "pCrWib7pTFW2"
   },
   "source": [
    "### Predict Function for variable length input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4GrjshKpPC8b",
   "metadata": {
    "id": "4GrjshKpPC8b"
   },
   "outputs": [],
   "source": [
    "def predict(lstm_model,input_string,device,type='tensor'):\n",
    "    if type!='tensor':\n",
    "        input_string_ints =  np.array([dna2int[i] for i in list(input_string)])\n",
    "        input_tensor = torch.from_numpy(input_string_ints).int()\n",
    "    else:\n",
    "        input_tensor = input_string\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequences = input_tensor\n",
    "        sequences = sequences.unsqueeze(0)\n",
    "        padded_sequences = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "        lengths = torch.LongTensor([len(seq) for seq in sequences])\n",
    "        packed_sequences = torch.nn.utils.rnn.pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "        data, lengths = pad_packed_sequence(packed_sequences, batch_first=True)\n",
    "        data = data.to(device)\n",
    "        predictions = lstm_model(data)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AC7CVIu6bi8m",
   "metadata": {
    "id": "AC7CVIu6bi8m"
   },
   "source": [
    "### Testing the predict function for a string input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "XAti_Xp8QIY2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAti_Xp8QIY2",
    "outputId": "85051f82-4f69-4df0-96c1-4c33e9b0aaa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0485]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = 'ANNNCGCGCGNGNNACGCGCNNCG'\n",
    "predict(model,input_string,'cpu',type='string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GCiv0g2vaqtB",
   "metadata": {
    "id": "GCiv0g2vaqtB"
   },
   "source": [
    "### Performance on the Testing Data using Mean Abosolute Error : 512 Samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "NNHWEyyYVvvs",
   "metadata": {
    "id": "NNHWEyyYVvvs"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "test_actuals = []\n",
    "for i in test_x:\n",
    "    input_tensor,input_target = i\n",
    "    model_prediction = predict(model,input_tensor,'cpu',type='tensor').item()\n",
    "    test_predictions.append(model_prediction)\n",
    "    actual = input_target.item()\n",
    "    test_actuals.append(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gS9S179mudiV",
   "metadata": {
    "id": "gS9S179mudiV"
   },
   "source": [
    "### Mean Absolute Error on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "_7sgavsHV5-y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7sgavsHV5-y",
    "outputId": "7c9f7b9c-7871-4308-cbc8-d437adf57e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8137505017220974"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(test_predictions,test_actuals) \n",
    "# Interpretation : On average the model prediction has the error of 1.81 compared to the actual target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "APi8GYYGYkIB",
   "metadata": {
    "id": "APi8GYYGYkIB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
